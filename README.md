# AI 생성 콘텐츠 진위 판별 시스템 개발
전남대학교 소중단 디지털 경진대회 참가 프로젝트입니다.  
이 프로젝트는 Hugging Face의 transformer 모델(team-lucid/deberta-v3-base-korean)을 파인튜닝하여 AI와 사람이 쓴 텍스트를 분류하는 이진 분류 문제를 해결하는 것을 목표로 합니다.

# 1. Introduction
본 프로젝트는 전남대학교 소중단 디지털 경진대회 참가를 위해 진행되었습니다. Python, transformers, datasets, scikit-learn 등의 라이브러리를 활용하여 한국어 텍스트가 AI에 의해 작성되었는지 인간이 작성했는지를 분류하는 모델을 개발합니다. Hugging Face의 team-lucid/deberta-v3-base-korean 사전학습 모델을 기반으로 파인튜닝을 진행하며, 데이터 전처리, 모델 학습, 평가, 그리고 실사용을 위한 배포까지의 전 과정을 다룹니다.

문제 정의 및 데이터 분석
# 문제 정의
생성형 AI의 보급으로 인해 AI가 작성한 글과 사람이 직접 작성한 글을 구분하는 기술의 필요성이 점차 커지고 있습니다. 특히 교육 환경에서는 학습자의 자율적인 사고와 표현 능력을 평가하기 위해, 과제물이나 에세이에서 AI의 개입을 최소화할 필요가 있습니다. 본 프로젝트에서는 주어진 텍스트가 인간에 의해 작성된 것인지, 혹은 AI가 생성한 것인지를 분류하는 이진 분류 문제(Binary Classification) 로 문제를 정의합니다.

# 사용한 데이터셋
본 프로젝트에서는 다음과 같은 데이터셋을 사용하였습니다.
사람이 작성한 텍스트: 국내 초·중·고 학생들이 실제로 작성한 자기소개서, 독후감, 수필 등의 에세이를 수집하였으며, 각 학령별로 글의 길이, 어휘 수준, 문체 등이 다양한 특징을 가집니다.
AI가 생성한 텍스트: 동일한 주제 또는 유사한 맥락에서 생성형 AI(ChatGPT 등)를 활용하여 생성한 텍스트를 수집하고, 주제나 문체가 사람 글과 최대한 유사하게 되도록 조정하여 학습 데이터에 포함시켰습니다.

# 데이터 전처리 및 EDA 결과
전처리 과정: 텍스트 내 불필요한 특수문자 제거, 띄어쓰기 정리, 중복 샘플 제거 등을 수행하였고, 학습 효율 향상을 위해 길이 제한(예: 최소 100자 이상)도 설정하였습니다.
탐색적 분석(EDA):전체 텍스트 길이 분포는 사람 작성 텍스트의 경우 평균 약 350자, AI 작성 텍스트의 경우 평균 약 400자 수준으로 확인되었습니다.
사람 글은 반복 표현, 감탄사, 개성 있는 문체 등 인간 특유의 언어 사용 패턴을 보이며, AI 텍스트는 더 정제된 문장 구성과 논리 구조가 강한 경향을 보입니다.
시각화를 통해 각 클래스별 단어 빈도수 차이, 문장 구조의 다양성 등을 비교 분석하였습니다.

# 분석 방향 설정
AI 텍스트와 인간 텍스트 간의 문체적·통계적 차이를 분석한 결과, 문맥의 자연스러움보다는 표현의 다양성, 비형식성, 감정의 직접성 등에서 차이가 도출됨을 확인하였습니다. 이에 따라 본 프로젝트는 문장 길이와 어휘 패턴, 표현 다양성 등의 특성을 반영할 수 있는 Transformer 계열 모델을 중심으로 텍스트 판별을 수행하고자 합니다. 특히 DeBERTa v3와 같이 문맥 기반의 의미 파악에 강점을 지닌 모델이 이 문제 해결에 적합하다고 판단하였습니다.

# 사용한 모델
모델명: team-lucid/deberta-v3-base-korean
기반 아키텍처: DeBERTa v3 (Decoding-enhanced BERT with disentangled attention)
특징:단어의 위치 정보(position)와 내용(content)을 분리하여 처리하는 disentangled attention 구조를 사용해 문맥 정보 파악 능력이 뛰어남
Enhanced Mask Decoder를 도입하여 마스킹된 위치의 복원 성능 향상
BERT나 RoBERTa 대비 문법적 정합성과 의미적 일관성 면에서 우수한 성능

# 모델 설계 이유와 구조 설명
본 문제는 단순 키워드 기반 분류가 아닌, 인간 고유의 감정 표현, 문체 다양성, 창의적 구성 등 고차원적 언어 특성을 판별해야 하므로, 문맥 이해력이 뛰어난 Transformer 모델이 적합하다고 판단
DeBERTa는 BERT보다도 정교한 attention 메커니즘을 갖추고 있어, 문장의 흐름과 구조, 표현의 자연스러움을 세밀하게 분석 가능
입력 문장은 토크나이징 후 최대 길이로 패딩되며, 마지막 레이어의 [CLS] 토큰 임베딩을 이진 분류기에 전달하여 AI 생성 / 사람 작성 여부를 예측하는 구조

# 학습 방법 및 하이퍼파라미터 설정
파인튜닝 전략: Pretrained된 DeBERTa v3 모델 위에 분류기 헤드(Linear Layer + Sigmoid)를 추가
Binary Cross Entropy Loss를 기준으로 역전파 및 최적화 수행
하이퍼파라미터 설정
배치 크기: 16
학습률: 2e-5
에폭 수: 3
옵티마이저: AdamW
학습 스케줄러: Linear Warmup Scheduler (warmup ratio 0.1)
Validation 방식: 학습 데이터셋의 20%를 검증 세트로 분할하여 성능 추적 및 오버피팅 방지


