# AI 생성 콘텐츠 진위 판별 시스템 개발
전남대학교 소중단 디지털 경진대회 참가 프로젝트
본 프로젝트는 Hugging Face의 Transformer 모델인 team-lucid/deberta-v3-base-korean을 파인튜닝하여, AI가 작성한 텍스트와 사람이 작성한 텍스트를 구분하는 이진 분류 문제를 해결하는 것을 목표로 합니다.
본 프로젝트는 전남대학교 소중단 디지털 경진대회 참가를 위해 진행하였습니다.
Python, transformers, datasets, scikit-learn 등의 라이브러리를 활용하여 한국어 텍스트가 AI에 의해 작성되었는지, 혹은 인간에 의해 작성되었는지를 분류하는 모델을 개발하였습니다.
Hugging Face의 team-lucid/deberta-v3-base-korean 사전학습 모델을 기반으로 파인튜닝을 진행하였으며, 데이터 전처리, 모델 학습, 성능 평가, 배포까지의 전 과정을 수행하였습니다.

# 1. 프로젝트 개요 및 주제 적합성
대회 주제: 생성형 AI(LLM)와 인간: 텍스트 판별 챌린지

문제를 선정하게 된 배경
최근 ChatGPT 등 생성형 AI의 확산으로 학생들이 글쓰기 과제에 AI를 적극적으로 활용하는 사례가 늘고 있습니다. 특히 초·중·고등학교의 기초 교육 과정에서 글쓰기는 단순히 결과물을 만드는 활동이 아니라, 문장 해독력과 문해력, 그리고 자신의 생각을 논리적으로 표현하는 능력을 기르는 중요한 과정입니다.
하지만 학생이 AI를 이용해 과제를 작성하면, 스스로 생각을 정리하고 언어적으로 표현하는 훈련이 제대로 이루어지지 않아, 장기적으로는 창의적 사고력과 자기표현 능력이 저하될 위험이 있습니다. 이러한 문제의식에서 출발하여, 실제로 학생이 직접 작성한 글과 AI가 생성한 글을 구별하는 기술 개발의 필요성이 대두되었습니다.

주제와의 연관성 및 적합성
본 프로젝트는 “생성형 AI(LLM)와 인간: 텍스트 판별 챌린지”라는 대회 주제에 매우 밀접하게 부합합니다. 실제 교육 현장에서는 학생의 평가와 성장 지원을 위해 제출된 과제가 인간의 자율적 사고와 표현에 기반한 것인지, 아니면 AI의 도움을 받은 것인지 판별해야 할 필요성이 점점 커지고 있습니다.
본 연구는 초·중·고 학생들이 작성한 자기소개서, 독후감, 수필 등 다양한 글쓰기 과제를 대상으로, AI 생성 여부를 판별하는 이진 분류 시스템을 개발합니다. 이를 통해 기초 교육에서의 문해력 저하, 사고력 훈련 부족 등 현실적인 사회적 이슈를 직접적으로 해결하고자 하며, 대회가 요구하는 “생성형 AI와 인간의 텍스트 구별”이라는 목표에 적합한 문제 설정입니다.

# 2. 문제 정의 및 데이터 분석
🔹 문제 정의
생성형 AI의 확산으로 인해 AI가 작성한 글과 사람이 작성한 글을 구별하는 기술의 중요성이 점차 부각되고 있습니다.
특히 교육 환경에서는 학습자의 자율적인 사고와 표현 능력을 평가하기 위해, AI 개입을 최소화할 수 있는 검증 메커니즘이 필요합니다.
따라서 본 프로젝트는 주어진 텍스트가 사람이 직접 작성했는지, AI가 생성했는지를 분류하는 **이진 분류 문제(Binary Classification)**로 정의하였습니다.

🔹 **사용한 데이터셋**

### 사람 작성 텍스트

- **출처:** AI Hub에서 제공하는 한국어 글쓰기 데이터셋 활용  
- **내용:** 국내 초·중·고등학생들이 실제로 작성한 다양한 유형의 에세이  
- **에세이 유형:** 찬성/반대, 글짓기, 대안 제시, 설명글, 주장글 등  
- **특징:**  
  - 학년별로 문체, 어휘 수준, 문장 구조, 글 길이에서 뚜렷한 차이를 보임  
  - 자연스럽고 비형식적인 문체가 포함되어 있음  

### AI 생성 텍스트

- **도구:** ChatGPT, Gemini 등 최신 생성형 AI 활용  
- **생성 조건:** 사람 에세이와 동일하거나 유사한 주제를 기반으로 생성  
- **조정 방식:**  
  - 학년과 주제 수준을 맞춰 문체와 어휘 수준을 조절  
  - 가능한 한 사람 작성 텍스트와 스타일이 유사하도록 튜닝  

🔹 데이터 전처리 및 EDA 결과
전처리 과정:
특수문자 제거, 띄어쓰기 정리, 중복 샘플 제거 등의 전처리를 수행하였으며, 학습 효율 향상을 위해 최소 길이 100자 제한을 설정하였습니다. 또한, 사용하는 모델인 BERT의 최대 토큰 수가 512개이므로, 입력 텍스트가 512 토큰을 초과하지 않도록 잘라내는 작업도 함께 진행하였습니다.
## 탐색적 분석(EDA)

본 프로젝트에서는 AI가 생성한 텍스트와 사람이 작성한 텍스트의 차이를 정량적·정성적으로 파악하기 위해 탐색적 데이터 분석(EDA)을 수행하였습니다.  
초기 분석에서는 두 클래스의 텍스트 길이, 어휘 다양성, 고빈도 단어 등 다양한 지표를 중심으로 비교하였으며, 시각화 결과를 통해 각 클래스의 특징이 뚜렷하게 드러남을 확인할 수 있었습니다.

사람 텍스트는 감탄사, 반복 표현, 감정적이고 자유로운 문체가 두드러진 반면, AI 텍스트는 논리적이고 정제된 구조, 일정한 길이, 반복적인 어휘 사용 등의 특성이 관찰되었습니다.  
이러한 차이는 아래의 구체적인 데이터 분석 결과와 각종 시각화를 통해 명확하게 확인할 수 있습니다.

## 데이터 분석 결과

### 1. 텍스트 길이 분포
![텍스트 길이 사진](https://github.com/user-attachments/assets/5116a52b-34a5-47bd-baa7-9e438768ec6e)

- **AI 텍스트**의 평균 길이(661.5)는 **Human 텍스트**(632.5)보다 소폭 길다.
- **표준편차는 Human(200.9)**이 **AI(144.8)**보다 크므로, 사람의 글이 AI보다 길이 측면에서 더 다양하게 분포되어 있음.
- 중앙값도 유사하나, Human 텍스트의 분산이 크기 때문에 일부 짧거나 긴 글이 더 존재할 수 있음.
- **AI는 상대적으로 일정한 길이의 텍스트를 생성하는 경향**이 있음.

### 2. 어휘 다양도
![어휘 다양도](https://github.com/user-attachments/assets/5703589a-c915-4cdc-9a34-104adad4644f)

- **Human 텍스트의 어휘 다양도:** 0.2029  
- **AI 텍스트의 어휘 다양도:** 0.0751  
- 사람이 사용하는 어휘가 훨씬 더 다양함.
- AI는 동일하거나 유사한 표현을 반복적으로 사용하는 경향이 있으며, 이는 특정 패턴이나 표현의 재사용 때문일 수 있음.
- 이 결과는 사람이 더 풍부한 언어 표현력을 보이고 있음을 시사함.

### 3. 사람(Human) 텍스트의 고빈도 단어
![사람 고빈도 단어](https://github.com/user-attachments/assets/c1fe8d29-7d94-4415-aee7-090b599618f1)

- ‘생각합니다’, ‘저는’, ‘나는’ 등 자기 주관적인 표현과 서술어 중심의 단어가 많이 나타남.
- "때문에", "있는", "있다" 등 논리적 연결어도 자주 사용되어 의사 표현이나 논리 전개가 드러남.
- 감정보다는 설명적이고 논리적인 글쓰기 방식이 강조되어 있음.

### 4. AI 텍스트의 고빈도 단어
![AI 고빈도 단어](https://github.com/user-attachments/assets/da660725-4885-4346-bfbd-bf678742e939)

- ‘같아요’, ‘정말’, ‘많은’ 등 감탄사나 정서적인 표현이 자주 등장함.
- AI가 회화체 중심의 텍스트 생성을 하며, 의견을 부드럽게 표현하려는 경향이 있음.
- ‘수’, ‘것’, ‘있는’, ‘있을’ 등 일반화된 표현과 가능성 표현이 빈번하게 사용됨. 이는 AI 모델이 모호성 회피를 위해 중립적인 어휘를 선호하는 경향 때문일 수 있음.

🔹 분석 방향 설정
문체적 특성과 감정 표현 등에서 사람과 AI 텍스트 간의 차이가 확인됨에 따라,
본 프로젝트는 표현 다양성, 비형식성, 문장 길이, 어휘 패턴 등 문맥적 특징을 학습할 수 있는 Transformer 계열 모델에 기반한 분류를 시도합니다.
특히 DeBERTa v3는 문맥 파악 능력이 뛰어나 이 문제 해결에 적합하다고 판단하였습니다.

# 3. 해결 아이디어 및 모델 구현
🔹 사용한 모델
모델명: team-lucid/deberta-v3-base-korean
기반 아키텍처: DeBERTa v3 (Decoding-enhanced BERT with disentangled attention)
모델 특징:
단어 위치와 내용을 분리하여 이해하는 Disentangled Attention 구조
Enhanced Mask Decoder를 활용해 마스킹 복원 성능을 강화
기존 BERT, RoBERTa 대비 정합성과 일관성이 우수함

🔹 모델 설계 이유와 구조
- 단순 키워드 기반이 아닌, 감정 표현, 문체 다양성, 창의성 등 고차원 언어 특성을 포착해야 하므로 문맥 이해력이 뛰어난 Transformer 계열 모델을 채택하였습니다.
- DeBERTa는 문맥 흐름과 표현의 자연스러움을 세밀하게 분석할 수 있어 본 문제에 적합합니다.
- 또한, 해당 모델은 한국어 데이터로 사전학습되어 한국어 텍스트 판별에 더욱 적합합니다.
- 모델 구조는 입력 텍스트를 토크나이징 후 패딩, 최종 레이어의 [CLS] 토큰을 이진 분류기에 전달하여 sigmoid로 AI 생성 여부를 예측합니다.

### 학습 및 실험 방법 요약

- **사전학습된 한국어 DeBERTa 모델 사용**
  - `team-lucid/deberta-v3-base-korean` 등 pretrained 모델을 불러와 이진 분류(사람/AI)로 파인튜닝

- **데이터 분할 및 셔플**
  - 학습/검증/테스트 데이터를 별도 CSV로 분할, split별로 seed 고정 후 셔플

- **토큰화 및 데이터셋 변환**
  - Huggingface `AutoTokenizer`로 최대 512 토큰 padding/truncation, Dataset 객체로 변환

- **Trainer 기반 학습**
  - Huggingface `Trainer`와 `TrainingArguments`로 학습/평가 자동화
  - 주요 하이퍼파라미터:
    - batch size: 16
    - epoch: 3
    - learning rate: 5e-5
    - weight decay: 0.01
    - evaluation & save strategy: epoch 단위
    - metric: f1
    - best model 자동 저장 및 불러오기
    - fp16(자동)
    - seed 고정

- **성능 평가 및 예측 결과 저장**
  - 검증/테스트셋에 대해 accuracy, precision, recall, f1 등 주요 지표 산출 및 로그 저장
  - 예측 결과를 CSV로 저장

- **실전 환경 대응**
  - Colab/로컬 환경 모두 동작, 예외처리/폴더 자동 생성/로깅 등 실무형 코드 구현

# 4. 모델 검증 및 성능 평가

### 🔍 평가 지표 설명

| 지표 (Metric) | 설명 |
|---------------|------|
| **Accuracy**  | 전체 예측 중 정답 비율. 전체적인 정확도. |
| **Precision** | 양성이라고 예측한 것 중 실제로 양성인 비율. 오탐(False Positive)에 민감. |
| **Recall**    | 실제 양성 중 모델이 맞춘 비율. 누락(False Negative)에 민감. |
| **F1-score**  | Precision과 Recall의 조화 평균. 두 지표 간의 균형을 평가. |

![b3a5d6d6-2626-4ca1-8236-819068c4aeaa](https://github.com/user-attachments/assets/009c04de-34f2-4aac-a791-7a6eaba143b3)

## 모델 성능 지표 비교

같은 모델 (`text_detector_output/checkpoint-750`)을 사용하여 두 개의 서로 다른 테스트셋에 대해 성능을 비교하였습니다.

# Dataset Information

## Easy Dataset
- 구성: GPT-3.5 Turbo 기반으로 생성된 문장들로 구성된 상대적으로 단순한 문장 패턴
- 샘플 수: 총 600개 (Human 300, AI 300)

## Hard Dataset
- 구성: 다양한 최신 LLM 기반으로 생성된 문장들이 포함
  - 사용된 모델:
    - GPT-4o, GPT-4o-mini, GPT-3.5-turbo
    - Claude 3.1 (Sonnet), Qwen 2.5 (0.5B)
    - LLaMA 3.1 8B, Zephyr, Gemini 등
- 샘플 수: 총 900개 (Human 450, AI 450)

> 이처럼 Hard Dataset은 다양한 스타일과 표현 방식이 혼합되어 있어 모델의 일반화 성능을 평가하는 데 더 적합합니다.

---

### ✅ 성능 비교

| **Dataset**       | Accuracy | Precision | Recall | F1-score |
|-------------------|----------|-----------|--------|----------|
| Easy Test Set     | 0.9983   | 1.0000    | 0.9967 | 0.9983   |
| Hard Test Set     | 0.9356   | 0.8904    | 0.9933 | 0.9391   |

> 📌 **해석**  
- **Easy Test Set**에서는 거의 완벽한 성능을 보임.  
- **Hard Test Set**에서는 **Recall은 유지**되었으나, **Precision이 감소**하여 오탐률 증가.  
- 실제 환경에서 신뢰할 수 있는 성능을 보장하기 위해서는 다양한 조건의 테스트셋을 통한 평가가 중요함.

### 📈 ROC AUC Score
![ROC 커브](https://github.com/user-attachments/assets/dae50bbe-93d1-482e-ac27-5c47937c88f2)

모델의 ROC AUC (Receiver Operating Characteristic - Area Under Curve) 값은 **0.9999**로 측정되었습니다.

> 이는 모델이 임의의 양성 샘플과 음성 샘플 쌍을 구분하는 능력이 **99.99%**에 달한다는 것을 의미합니다.

#### ✅ 해석
- ROC AUC 값은 **1에 가까울수록 이상적이며**, 0.9999는 거의 완벽한 분류 성능을 뜻합니다.
- 이는 모델이 다양한 **threshold 조건**에서도 양성과 음성을 거의 완벽하게 구분할 수 있음을 의미합니다.
- 특히 **민감도(Recall)**와 **특이도(Specificity)** 사이의 **균형이 매우 잘 잡혀** 있다는 신호로 해석할 수 있습니다.

### 🔹 성능 해석 및 분석

이번 실험에서는 다양한 난이도의 데이터셋을 활용하여 `DeBERTa v3` 기반 분류 모델의 성능을 평가하였습니다. 동일한 모델에 대해 GPT 기반의 단순한 텍스트로 구성된 **Easy Dataset**과, 다양한 최신 LLM(GPT-4o, Claude, LLaMA 3, Gemini 등)으로 생성된 복잡한 문장이 포함된 **Hard Dataset**에 대해 비교 분석하였습니다.

- **Easy Dataset**에서 F1-score는 **0.9983**, ROC AUC는 **0.9999**로 거의 완벽에 가까운 분류 성능을 보였습니다.
- **Hard Dataset**에서는 F1-score가 **0.9391**로 다소 하락하였지만, 여전히 높은 **Recall (0.9933)**을 유지하여 **AI 생성 문장을 놓치지 않고 탐지하는 능력**이 매우 우수함을 나타냈습니다.

#### 주요 해석 포인트
- `DeBERTa v3`는 문장의 구조적 다양성과 반복적 표현, 고차원 언어 패턴을 효과적으로 학습하여 기존 BoW, TF-IDF+SVM 기반 접근법 대비 성능이 **압도적으로 우수**했습니다.
- **BoW 및 TF-IDF+SVM 모델**은 단어 단위 정보만을 반영해 문맥 파악에 한계가 있었고, 이에 따라 AI 생성 문장과 사람 작성 문장 간 **표현상의 미세한 차이**를 구분하지 못했습니다.
- 반면, `DeBERTa v3`는 Transformer 기반 구조를 통해 이러한 미묘한 언어적 차이를 **정확히 포착**할 수 있었고, 특히 **Recall이 높다는 점**은 실제 AI 탐지 시스템으로 활용할 때 **위험한 문장을 놓치지 않고 잡아내는 능력**이 뛰어남을 의미합니다.

> 📌 결론적으로, 본 모델은 **AI 생성 문장 탐지 과제에 매우 적합하며**,  
> 실제 교육 환경, AI 윤리 필터링, 자동 에세이 판별 시스템 등에 적용하기에 충분한 정밀성과 일반화 성능을 확보하고 있습니다.

# 5. 한계점 및 개선 방향
# 프로젝트 진행 중 겪은 한계 및 개선 방향

---

## 🔹 프로젝트 진행 중 겪은 한계

### 이진 분류 방식의 한계점

- **복잡한 언어 현상 단순화**  
  자연어 문장은 단순히 ‘Human’ 또는 ‘AI’로 명확히 나뉘기 어렵습니다. AI가 생성했지만 매우 자연스러운 문장도 있고, 인간 작성이지만 기계적이고 어색한 문장도 많아 경계가 모호합니다.  
  이진 분류는 이러한 미묘한 스펙트럼을 반영하지 못해 정보 손실이 발생합니다.

- **중간 형태의 문장 처리 한계**  
  AI가 부분적으로 편집한 인간 문장, 혹은 인간과 AI가 협업한 문장 등 중간 상태를 구분하지 못합니다.  
  두 클래스로 강제 분류하면서 오류 가능성이 커집니다.

- **다양한 AI 모델 구분 불가**  
  GPT-4o, Claude, Qwen 등 여러 LLM이 혼재된 데이터셋에서 단순히 ‘AI’ 클래스로 묶으면 모델별 특성이나 스타일 차이를 반영할 수 없습니다.  
  따라서 세분화된 분석이나 맞춤형 대응이 어렵다는 한계가 있습니다.

- **실제 활용과 평가 목적 불일치 가능성**  
  AI/인간 구분에만 집중하면 문장 품질 평가, 감정 분석, 내용 이해 등 다양한 NLP 태스크에 부적합할 수 있습니다.  
  이진 분류는 목적에 비해 지나치게 제한적인 시각을 제공합니다.

- **변화하는 AI 생성 특성 반영 어려움**  
  AI 생성 문장은 시간이 지날수록 발전하고 변화합니다.  
  고정된 이진 라벨링은 최신 생성 스타일을 포착하기 어렵고, 모델 적응력을 저해할 수 있습니다.

---

## 🔹 개선이 필요한 부분 및 추가 보완 아이디어

- **다양한 AI 생성기 반영**  
  Claude, KoAlpaca, KoGPT 등 다양한 생성 모델을 활용해 동일 주제로 텍스트를 생성하고, 이를 학습 데이터에 포함하여 문체 다양성을 반영할 예정입니다.  
  _예시: ‘기후 변화’ 주제로 GPT-3.5와 KoAlpaca가 작성한 결과 비교_

- **형식적 피처 기반 보조 모델 설계**  
  문장 평균 길이, 조사 사용 빈도, 단어 다양도(type-token ratio), 맞춤법 오류 수 등 언어적 스타일 지표를 정량화하여 부가 입력값으로 활용하는 방법을 고려하고 있습니다.

- **다중 분류 또는 연속형 예측 모델 전환**  
  단순 이진 분류 대신 ‘AI 생성 가능성 점수’를 0~1 사이로 예측하거나, GPT, Claude, KoGPT 등 모델별 분류로 확장하는 방안을 검토 중입니다.

- **세분화된 라벨링 및 연령 고려**  
  초등학생 작성 글은 문체가 간단하고 반복적이므로, 학년별 라벨링 및 사람 텍스트 내 분류 기준을 강화하여 성능 개선을 목표로 합니다.  
  _예시:_  
  - 초등학생: “나는 강아지를 좋아한다. 귀엽기 때문이다.”  
  - 고등학생: “반려동물은 정서적 안정에 기여하며, 사회적 책임도 수반된다.”

---

# 6. 기대 가능성 및 활용 방안
🔹 본선 진출 가능성 및 차별성 요소
본 프로젝트는 단순 모델 구현에 그치지 않고, 실제 한국어 교육 현장에서 발생할 수 있는 문제 상황을 구체적으로 가정하여, 현실 기반의 문제 해결 모델을 제시하였습니다.
기존 텍스트 분류 문제에서 다루지 않던 ‘문체의 인간성’이라는 비정형적 특성을 정량적으로 탐지한다는 점에서 차별성이 있습니다.
Hugging Face 모델을 커스터마이징하고, 실사용 예시까지 제시하였다는 점에서 적용 가능성과 완성도 면에서 높은 평가를 받을 수 있다고 기대합니다.

🔹 실용성 및 확장 가능성
교육 분야 활용:
교사나 평가자는 제출된 과제가 AI의 개입 없이 작성된 것인지 자동으로 탐지할 수 있으며, 자기주도학습의 신뢰성과 질을 확보하는 데 기여할 수 있습니다.
출판 및 저작물 검증:
블로그, 칼럼, 기사 등에서 AI 생성 콘텐츠가 무단 활용될 경우, 해당 텍스트의 진위를 확인하는 표절 방지 및 저작권 보호 수단으로 활용될 수 있습니다.
산업적 응용:
기업에서는 고객 응대, 리뷰 분석, SNS 분석 등에서 AI 작성 콘텐츠와 실제 사용자 반응을 구분하는 데 본 시스템을 활용할 수 있습니다.

🔹 향후 연구 또는 개선 아이디어
Zero-shot / Few-shot 분류 실험:
사전학습된 거대 언어모델(GPT-4 등)을 활용한 Zero-shot 평가 모델과의 비교를 통해, 현재 모델의 위치와 경쟁력을 진단할 예정입니다.
Hybrid 분류 체계 설계:
Transformer 기반 분류기와 통계 기반 탐지기를 병렬로 적용하거나, 신뢰도 기반 앙상블 기법을 활용하여 예측 정확도를 높이는 방향도 연구할 수 있습니다.
웹 서비스화:
모델을 REST API 혹은 웹 앱 형태로 배포하여, 교사, 평가자, 일반 사용자 모두가 쉽게 활용할 수 있는 서비스 형태로 제공하는 것을 장기적인 목표로 설정합니다.

